---
published : true # 건드리지 마세요
layout : posts # 건드리지 마세요
title: "[ML Research] 강화학습 심화(2) " # [category 명]제목  <= 해당 양식에 맞춰주세요
date: 2020-02-24 # 날짜를 적어주세요
excerpt: "Temproal difference와 on-policy, off policy의 차이" # 요약을 적어주세요
comments: true # 건드리지 마세요
use_math: true # 건드리지 마세요
author : Jeongwon jo # 본인 프로필 이름을 입력해주세요
---

# 3-3 Temporal difference
MC는 학습할 때 episode가 끝나야만 한다는 단점이 있어 episode가 긴 학습에 대해 비효율적입니다. 따라서 step마다 학습이 가능한 TD를 사용합니다. TD는 기본적으로 MC와 같은 방법으로 sample을 합니다. MC method가 한 epsiode가 끝나야 return값을 받아내어 학습하지만 TD method는 다음 step까지만 기다려서 target을 유추합니다.



# 3-4 MC vs TD

* MC
  - unbiased
  - high variance

* TD
  - biased
  - low variance

bias와 variance는 보통 한쪽이 낮아지면 다른 한쪽이 높아지게 됩니다. TD는 다음 state에만 영향을 받기 때문에 편향적이지만 변동이 작습니다. 반대로 MC는 epsiode가 끝나야지만 학습하기 때문에 여러개의 step을 모두 반영하여 높은 variance를 가지게 됩니다.

# 4-1 On-policy vs Off-policy

SARSA와 Q-learning은 모두 TD알고리즘을 사용합니다. TD내에서도 구분이 있는데 SARSAS는 on-plolicy, Q-learning은 off-policy를 사용합니다. 



on-policy는 behavior policy와 target policy가 같은 것을 의미하고 off-policy는 다른 것은 의미합니다. 위의 수식에서 $p(a_{t+1}|s_{t+1})$ TD target을 만들기 위해서 뽑아야하는 policy로 target policy라 하고 , $p(s_{t+1}|s_t,a_t)$는 실제로 행동하는 policy로 behavior policy라 합니다.



* off-policy를 쓰는 이유
  * 사람 or other agents
  * 실컷 탐험하면서 optimal policy(Greedy)로 sample(TD-target) 얻는다.
  * 재평가 가능


Q-learning에서 behavior policy와 target policy를 분리해 사용할 때 가지는 장점이 있습니다. 첫 번째로 다른 agent가 사용한 policy를 target policy로 사용할 수 있다는 점입니다. 두 번째로 실제 action을 하는데는 ε-greedy를 해야하지만 학습에 사용할 다음 state에 대해서는 greedy한 action을 할 수 있다는 점입니다. 마지막으로 재평가가 가능하다는 점입니다. 



# 4-2 Q-learning(심화편)




target : 

behavior = ε-greedy

target policy는 $a_{t+1}^*$를 뽑는 greedy한 action을 하고, behabior는 ε-greedy action을 합니다.


target policy를 δ함수로 바꾸어 줍니다. 그리고 δ함수를 적분한다는 것은 $a_{t+1}^*$을 대입하는 것이고 γ$\underset{a_t+1}{maxQ}$를 구하는 식이 나오게 됩니다. 이 식은 N번 sample한 것에 의해 나타나는 값이 되고 $R_t^{(N)} + $γ$\underset{a_t+1}{max}$$Q(s_{t+1}^{(N)},a_{t+1}^{(N)})$는 Q-learning의 TD-target이 됩니다.



# 4-3 SARSA vs Q-learning

위 그림에서 SARSA와 Q-Learning 각각의 방식으로 학습을 한다고 했을 때 SARSA는 절벽 근처의 경로에는 낮은 reward값을 갖게 되어 멀리 돌아가는 방식으로 학습합니다. 반면 Q-learning은 ε-greedy 방식으로 움직이기 때문에 절벽에서 가까운 최단 경로를 찾을 수 있습니다.





# 4-4 n-step TD vs n-step Q-learning



TD target : 


2-step TD에 대한 수식입니다. n-step TD는 MC와 TD의 장점을 모두 가진 방법입니다. 

## Reference
www.youtube.com/watch?v=cvctS4xWSaU&list=PL_iJu012NOxehE8fdF9me4TLfbdv3ZW8g﻿


